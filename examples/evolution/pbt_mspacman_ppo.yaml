alg_para:
  alg_name: PPO

env_para:
  env_name: AtariEnv
  env_info:
    name: MsPacmanNoFrameskip-v4
    vision: False

agent_para:
  agent_name: AtariPpo
  agent_num : 1
  agent_config:
    max_steps: 128
    complete_step: 50000000

model_para:
  actor:
    model_name: PpoCnn
    state_dim: [84, 84, 4]
    action_dim: 9
    input_dtype: uint8
    model_config:
      BATCH_SIZE: 320
      CRITIC_LOSS_COEF: 1.0
      ENTROPY_LOSS: 0.003
      LOSS_CLIPPING: 0.1
      LR: 0.0002
      MAX_GRAD_NORM: 5.0
      NUM_SGD_ITER: 4
      VF_SHARE_LAYERS: True
      activation: relu
      hidden_sizes: [256]


env_num: 2
speedup: False

# population based training configuration
use_pbt: True
pbt_config:
  population_size: 25
  pbt_interval: 400000
  metric_key: mean_episodic_reward
  hyperparameters_mutations:
    LR: [0.0006, 0.0005, 0.0004, 0.0003, 0.0002, 0.0001]
    CRITIC_LOSS_COEF: [1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4]
    ENTROPY_LOSS: [0.01, 0.008, 0.007, 0.006, 0.005, 0.004, 0.003, 0.002, 0.001]
    LOSS_CLIPPING: [0.5, 0.4, 0.3, 0.25, 0.20, 0.15, 0.1, 0.05]
    NUM_SGD_ITER: [4, 5, 6, 7, 8, 9, 10]

benchmark:
  log_interval_to_train: 400
